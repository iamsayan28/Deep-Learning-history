{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "box2D is not installed, run `pip install gym[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/box2d/bipedal_walker.py:14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         circleShape,\n\u001b[1;32m     17\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         revoluteJointDef,\n\u001b[1;32m     22\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 147\u001b[0m\n\u001b[1;32m    144\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m trained_agent, scores, epsilons \u001b[38;5;241m=\u001b[39m train_agent(episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m    150\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrocket_landing_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 71\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(episodes, batch_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_agent\u001b[39m(episodes, batch_size):\n\u001b[0;32m---> 71\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m     state_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     73\u001b[0m     action_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m load(spec_\u001b[38;5;241m.\u001b[39mentry_point)\n\u001b[1;32m    583\u001b[0m mode \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m apply_human_rendering \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/registration.py:61\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Calls the environment constructor\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(mod_name)\n\u001b[1;32m     62\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1310\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/box2d/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/box2d/bipedal_walker.py:24\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         circleShape,\n\u001b[1;32m     17\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         revoluteJointDef,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox2D is not installed, run `pip install gym[box2d]`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Define the Agent\n",
    "class RocketLandingAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.gamma = 0.99  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        act_values = self.model(state)\n",
    "        return np.argmax(act_values.cpu().data.numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = torch.FloatTensor([i[0] for i in minibatch]).to(self.device)\n",
    "        actions = torch.LongTensor([i[1] for i in minibatch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([i[2] for i in minibatch]).to(self.device)\n",
    "        next_states = torch.FloatTensor([i[3] for i in minibatch]).to(self.device)\n",
    "        dones = torch.FloatTensor([i[4] for i in minibatch]).to(self.device)\n",
    "\n",
    "        state_action_values = self.model(states).gather(1, actions.unsqueeze(-1))\n",
    "        next_state_values = self.model(next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * self.gamma) * (1 - dones) + rewards\n",
    "\n",
    "        loss = nn.SmoothL1Loss()(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Training the agent with visualization\n",
    "def train_agent(episodes, batch_size):\n",
    "    env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = RocketLandingAgent(state_size, action_size)\n",
    "    \n",
    "    scores = []\n",
    "    epsilons = []\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        epsilons.append(agent.epsilon)\n",
    "        \n",
    "        print(f\"Episode: {e+1}/{episodes}, Score: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "        \n",
    "        if (e + 1) % 50 == 0:\n",
    "            plot_progress(e + 1, scores, epsilons)\n",
    "            visualize_episode(agent, env, e + 1)\n",
    "    \n",
    "    return agent, scores, epsilons\n",
    "\n",
    "# Function to plot progress\n",
    "def plot_progress(episode, scores, epsilons):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(scores)\n",
    "    plt.title(f'Score vs Episode (Episode {episode})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilons)\n",
    "    plt.title(f'Epsilon vs Episode (Episode {episode})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'progress_episode_{episode}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Function to visualize a single episode\n",
    "def visualize_episode(agent, env, episode):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    frames = []\n",
    "    \n",
    "    while not done:\n",
    "        frames.append(env.render())\n",
    "        action = agent.act(state)\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(min(16, len(frames))):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(frames[i])\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f'Episode {episode} Visualization')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualization_episode_{episode}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Train the agent\n",
    "trained_agent, scores, epsilons = train_agent(episodes=1000, batch_size=64)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_agent.model.state_dict(), 'rocket_landing_model.pth')\n",
    "\n",
    "# Plot final results\n",
    "plot_progress(1000, scores, epsilons)\n",
    "\n",
    "print(\"Training completed. Check the progress plots and episode visualizations saved as PNG files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/saurabh/projects/env/lib/python3.12/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/saurabh/projects/env/lib/python3.12/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/saurabh/projects/env/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Installing collected packages: gym\n",
      "Successfully installed gym-0.26.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "box2D is not installed, run `pip install gym[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/box2d/bipedal_walker.py:14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         circleShape,\n\u001b[1;32m     17\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         revoluteJointDef,\n\u001b[1;32m     22\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/Box2D/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# C++ version copyright 2010 Erin Catto http://www.gphysics.com\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 3. This notice may not be removed or altered from any source distribution.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBox2D\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     21\u001b[0m __author__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$Date$\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/Box2D/Box2D.py:32\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _mod\n\u001b[0;32m---> 32\u001b[0m _Box2D \u001b[38;5;241m=\u001b[39m swig_import_helper()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m swig_import_helper\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/Box2D/Box2D.py:19\u001b[0m, in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dirname\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimp\u001b[39;00m\n\u001b[1;32m     20\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 159\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Main script\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLanderContinuous-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m     state_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    162\u001b[0m     action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m load(spec_\u001b[38;5;241m.\u001b[39mentry_point)\n\u001b[1;32m    583\u001b[0m mode \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m apply_human_rendering \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/registration.py:61\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Calls the environment constructor\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(mod_name)\n\u001b[1;32m     62\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1310\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/box2d/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[0;32m~/projects/env/lib/python3.12/site-packages/gym/envs/box2d/bipedal_walker.py:24\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         circleShape,\n\u001b[1;32m     17\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         revoluteJointDef,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox2D is not installed, run `pip install gym[box2d]`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Decision Transformer model (same as before)\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_length, hidden_size, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.state_encoder = nn.Linear(state_dim, hidden_size)\n",
    "        self.action_encoder = nn.Linear(action_dim, hidden_size)\n",
    "        self.return_encoder = nn.Linear(1, hidden_size)\n",
    "\n",
    "        self.embed_timestep = nn.Embedding(max_length, hidden_size)\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.action_predictor = nn.Linear(hidden_size, action_dim)\n",
    "\n",
    "    def forward(self, states, actions, returns_to_go, timesteps):\n",
    "        batch_size, seq_length = states.shape[0], states.shape[1]\n",
    "\n",
    "        state_embeddings = self.state_encoder(states)\n",
    "        action_embeddings = self.action_encoder(actions)\n",
    "        returns_embeddings = self.return_encoder(returns_to_go.unsqueeze(-1))\n",
    "\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "\n",
    "        sequence = torch.stack(\n",
    "            [returns_embeddings, state_embeddings, action_embeddings], dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(batch_size, 3 * seq_length, self.hidden_size)\n",
    "        \n",
    "        sequence = sequence + time_embeddings\n",
    "\n",
    "        transformer_outputs = self.transformer(sequence)\n",
    "\n",
    "        action_preds = self.action_predictor(transformer_outputs[:, 1::3])\n",
    "\n",
    "        return action_preds\n",
    "\n",
    "# Function to collect trajectories\n",
    "def collect_trajectories(env, num_trajectories):\n",
    "    trajectories = []\n",
    "    for _ in range(num_trajectories):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        trajectories.append((trajectory, total_reward))\n",
    "    return trajectories\n",
    "\n",
    "# Function to prepare data for training\n",
    "def prepare_data(trajectories, max_length):\n",
    "    states, actions, returns_to_go, timesteps = [], [], [], []\n",
    "    for trajectory, total_reward in trajectories:\n",
    "        traj_states, traj_actions, traj_returns = [], [], []\n",
    "        for t, (s, a, r) in enumerate(trajectory):\n",
    "            traj_states.append(s)\n",
    "            traj_actions.append(a)\n",
    "            traj_returns.append(total_reward - sum(r for _, _, r in trajectory[:t]))\n",
    "        \n",
    "        # Pad trajectories to max_length\n",
    "        traj_length = len(traj_states)\n",
    "        if traj_length < max_length:\n",
    "            padding = max_length - traj_length\n",
    "            traj_states += [np.zeros_like(traj_states[0])] * padding\n",
    "            traj_actions += [np.zeros_like(traj_actions[0])] * padding\n",
    "            traj_returns += [0] * padding\n",
    "        else:\n",
    "            traj_states = traj_states[:max_length]\n",
    "            traj_actions = traj_actions[:max_length]\n",
    "            traj_returns = traj_returns[:max_length]\n",
    "        \n",
    "        states.append(traj_states)\n",
    "        actions.append(traj_actions)\n",
    "        returns_to_go.append(traj_returns)\n",
    "        timesteps.append(list(range(max_length)))\n",
    "    \n",
    "    return (torch.FloatTensor(states),\n",
    "            torch.FloatTensor(actions),\n",
    "            torch.FloatTensor(returns_to_go),\n",
    "            torch.LongTensor(timesteps))\n",
    "\n",
    "# Training loop\n",
    "def train(model, env, num_epochs, batch_size, lr, num_trajectories, max_length):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        trajectories = collect_trajectories(env, num_trajectories)\n",
    "        states, actions, returns_to_go, timesteps = prepare_data(trajectories, max_length)\n",
    "        \n",
    "        for i in range(0, len(states), batch_size):\n",
    "            batch_states = states[i:i+batch_size]\n",
    "            batch_actions = actions[i:i+batch_size]\n",
    "            batch_returns = returns_to_go[i:i+batch_size]\n",
    "            batch_timesteps = timesteps[i:i+batch_size]\n",
    "            \n",
    "            action_preds = model(batch_states, batch_actions, batch_returns, batch_timesteps)\n",
    "            loss = loss_fn(action_preds, batch_actions)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, env, max_length):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    states, actions, returns_to_go, timesteps = [], [], [], []\n",
    "    \n",
    "    for t in range(max_length):\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        states.append(state)\n",
    "        returns_to_go.append(total_reward)\n",
    "        timesteps.append(t)\n",
    "        \n",
    "        if len(states) < max_length:\n",
    "            actions.append(np.zeros(env.action_space.shape[0]))\n",
    "        \n",
    "        model_input = prepare_data([(list(zip(states, actions, returns_to_go)), total_reward)], max_length)\n",
    "        action_pred = model(*[tensor.unsqueeze(0) for tensor in model_input]).squeeze(0)[-1]\n",
    "        \n",
    "        action = action_pred.detach().numpy()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_length = 1000\n",
    "    hidden_size = 128\n",
    "    num_heads = 8\n",
    "    num_layers = 3\n",
    "    \n",
    "    model = DecisionTransformer(state_dim, action_dim, max_length, hidden_size, num_heads, num_layers)\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs = 50\n",
    "    batch_size = 64\n",
    "    lr = 1e-4\n",
    "    num_trajectories = 100\n",
    "    \n",
    "    # Train the model\n",
    "    train(model, env, num_epochs, batch_size, lr, num_trajectories, max_length)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    num_eval_episodes = 10\n",
    "    eval_rewards = [evaluate(model, env, max_length) for _ in range(num_eval_episodes)]\n",
    "    \n",
    "    print(f\"Average evaluation reward: {np.mean(eval_rewards):.2f}\")\n",
    "    \n",
    "    # Visualize a single episode\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    states, actions, returns_to_go, timesteps = [], [], [], []\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        states.append(state)\n",
    "        returns_to_go.append(total_reward)\n",
    "        timesteps.append(len(states) - 1)\n",
    "        \n",
    "        if len(states) < max_length:\n",
    "            actions.append(np.zeros(env.action_space.shape[0]))\n",
    "        \n",
    "        model_input = prepare_data([(list(zip(states, actions, returns_to_go)), total_reward)], max_length)\n",
    "        action_pred = model(*[tensor.unsqueeze(0) for tensor in model_input]).squeeze(0)[-1]\n",
    "        \n",
    "        action = action_pred.detach().numpy()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"Visualization episode reward: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
