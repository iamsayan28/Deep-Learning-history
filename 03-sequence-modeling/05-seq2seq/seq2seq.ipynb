{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq \n",
    "\n",
    "## background\n",
    "\n",
    "### the problem with traditional neural networks\n",
    "\n",
    "traditional neural networks like anns (artificial neural networks) and cnns (convolutional neural networks) weren't cutting it for text data.\n",
    "\n",
    "Two main significant reasons : \n",
    "\n",
    "- **fixed input size**: these models typically expect a fixed input size, which doesn't work well for variable-length sequences like sentences.\n",
    "- **lack of temporal understanding**: they don't naturally capture the order and context of words in a sequence.\n",
    "\n",
    "### rnns (recurrent neural networks)\n",
    "\n",
    "rnns were introduced to handle sequential data better. they process input sequentially, maintaining a hidden state that can capture some context. however, they had their own issues:\n",
    "\n",
    "- **vanishing gradient problem**: as the sequence gets longer, rnns struggle to carry information from earlier time steps.\n",
    "- **limited context**: they have trouble capturing long-term dependencies in the data.\n",
    "\n",
    "### lstm (kind of solve)\n",
    "\n",
    "long short-term memory (lstm) networks were design to solve the RNN problem \n",
    "\n",
    "- **gating mechanisms**: lstms use gates to control the flow of information, helping to mitigate the vanishing gradient problem.\n",
    "- **better at long-term dependencies**: they can carry relevant information across longer sequences.\n",
    "\n",
    "but even lstms (and their variants like grus - gated recurrent units) struggle with very long sequences.\n",
    "\n",
    "## seq2seq: \n",
    "\n",
    "seq2seq (sequence-to-sequence) models were designed to handle tasks where both input and output are sequences, like machine translation.\n",
    "\n",
    "### core idea\n",
    "\n",
    "the seq2seq model consists of two main parts:\n",
    "\n",
    "1. **encoder**: processes the input sequence\n",
    "2. **decoder**: generates the output sequence\n",
    "\n",
    "this architecture allows the model to map sequences of different lengths, which is crucial for tasks like translation where input and output lengths may vary.\n",
    "\n",
    "### how seq2seq works\n",
    "\n",
    "let's break down the process:\n",
    "\n",
    "1. **input processing**:\n",
    "   - text input is tokenized (split into words or subwords)\n",
    "   - tokens are converted to numerical representations via an embedding layer\n",
    "\n",
    "2. **encoding**:\n",
    "   - the embedded input sequence is fed into the encoder (usually lstm-based)\n",
    "   - encoder processes the sequence, updating its hidden state at each step\n",
    "   - final hidden state of the encoder captures the essence of the input sequence\n",
    "\n",
    "3. **context vector**:\n",
    "   - the final hidden state of the encoder becomes the \"context vector\"\n",
    "   - this vector is meant to encapsulate the meaning of the entire input sequence\n",
    "\n",
    "4. **decoding**:\n",
    "   - decoder initializes its hidden state with the context vector\n",
    "   - at each step, the decoder:\n",
    "     - takes the previous output and its current hidden state as input\n",
    "     - produces a probability distribution over the output vocabulary\n",
    "     - selects the most likely token as the output for that step\n",
    "\n",
    "5. **output generation**:\n",
    "   - the process continues until the decoder generates an end-of-sequence token or reaches a maximum length\n",
    "\n",
    "### IMP finding in seq2seq\n",
    "\n",
    "1. **separate encoder and decoder**:\n",
    "   - allows handling different languages or domains for input and output\n",
    "   - enables more parameters without excessive computational cost\n",
    "   - can be trained separately, adding flexibility\n",
    "\n",
    "2. **deep lstms**:\n",
    "   - stacking multiple lstm layers (typically 4) in both encoder and decoder\n",
    "   - increases model capacity to capture complex patterns\n",
    "   - helps maintain long-term dependencies\n",
    "\n",
    "3. **input reversal**:\n",
    "   - reversing the order of input tokens (but not output tokens)\n",
    "   - creates shorter dependencies between source and target\n",
    "   - makes optimization easier for gradient-based methods like sgd\n",
    "\n",
    "4. **attention mechanism** (a later addition):\n",
    "   - allows decoder to focus on different parts of input for each output token\n",
    "   - significantly improves performance, especially for long sequences\n",
    "   - paved the way for transformer models\n",
    "\n",
    "\n",
    "### beam search decoding\n",
    "\n",
    "instead of greedily selecting the most probable token at each step, beam search maintains multiple candidate sequences:\n",
    "\n",
    "- keeps top-k most likely sequences at each step\n",
    "- improves output quality by exploring more possibilities\n",
    "\n",
    "### handling unknown words\n",
    "\n",
    "seq2seq models struggle with words not in their vocabulary. solutions include:\n",
    "\n",
    "- subword tokenization (e.g., byte-pair encoding)\n",
    "- pointer-generator networks for copying unknown words from input\n",
    "\n",
    "### bidirectional encoders\n",
    "\n",
    "using bidirectional lstms in the encoder to capture context from both directions of the input sequence.\n",
    "\n",
    "\n",
    "## limitations \n",
    "\n",
    "\n",
    "- still struggle with very long sequences\n",
    "- computationally intensive, especially during training\n",
    "- require large amounts of parallel data for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
