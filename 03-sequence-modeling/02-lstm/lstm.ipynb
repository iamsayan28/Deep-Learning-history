{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduction\n",
    "\n",
    "- this is the updated version of rnn\n",
    "\n",
    "- it solve the gradient vashing and gradient exploding problems.\n",
    "\n",
    "- it introduces gating mechanisms (input gate, forget gate, output gate) to control the flow of information\n",
    "- these gates allow the network to selectively remember or forget information over long sequences\n",
    "- LSTMs can capture long-term dependencies more effectively than traditional RNNs\n",
    "- they are widely used in various sequence modeling tasks like language modeling, machine translation, and speech recognition\n",
    "\n",
    "![LSTM](images/lstm.png)\n",
    "\n",
    "### how they work internally ?\n",
    " \n",
    "1. input gate: decides which takes the input for cell\n",
    "   - uses sigmoid activation to output values between 0 and 1\n",
    "   - decides which values to update\n",
    "\n",
    "![input](images/lstm-input.png)\n",
    "\n",
    "2. forget gate: decides what informantion should get removed from the cell \n",
    "    - also uses sigmoid activation\n",
    "    - outputs values close to 0 (forget) or 1 (keep)\n",
    "\n",
    "![forget](images/lstm-2.png)\n",
    "\n",
    "3. cell state update: combines the forget gate and input gate outputs\n",
    "   - multiplies the old cell state by the forget gate output\n",
    "   - adds the input gate output multiplied by a candidate cell state\n",
    "\n",
    "![add](images/lstm-add.png)\n",
    "\n",
    "4. output gate: \n",
    "   - uses sigmoid activation\n",
    "   - filters the updated cell state to produce the final output\n",
    "\n",
    "![last](images/last-lstm.png)\n",
    "\n",
    "5. final output: combines the output gate with the updated cell state\n",
    "   - the cell state is passed through a tanh function and multiplied by the output gate\n",
    "\n",
    "this architecture allows LSTMs to maintain relevant information over long sequences,\n",
    "selectively update or forget information, and produce relevant outputs at each time step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
