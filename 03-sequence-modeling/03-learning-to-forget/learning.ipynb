{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backgroung \n",
    "\n",
    "rnn suffers from the vanishing or exploding gradients problem and some them porblem solved by std lstm to some extend.\n",
    "lstms creats the bridge which help in solving the context problems through the cells.\n",
    "\n",
    "but lstms may suffers in input seq get too big. there is chance the magnitude of thst bridge carries may lead overflow and eventually failed. the proposed solution is an adaptive \"forget gate\" that allows an lstm cell to learn to reset itself at appropriate times, releasing internal resources.\n",
    "\n",
    "## standard lstm\n",
    "\n",
    "the basic unit in the hidden layer of an lstm network is the memory block, which contains:\n",
    "- one or more memory cells\n",
    "- a pair of adaptive, multiplicative gating units (input and output gates)\n",
    "\n",
    "each memory cell has a recurrently self-connected linear unit called the \"constant error carousel\" (cec), which helps prevent the vanishing gradient problem. the cell state, denoted as $s_c$, is updated as follows:\n",
    "\n",
    "$$\n",
    "s_{c}(t) = s_{c}(t-1) + y_{in}(t) \\cdot g(\\text{net}_{c}(t))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y_{in}(t)$ is the input gate activation\n",
    "- $g(\\cdot)$ is a centered logistic sigmoid function with range $[-2, 2]$\n",
    "\n",
    "the cell output $y_c$ is calculated as:\n",
    "\n",
    "$$\n",
    "y_{c}(t) = y_{out}(t) \\cdot h(s_{c}(t))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y_{out}(t)$ is the output gate activation\n",
    "- $h(\\cdot)$ is a centered sigmoid function with range $[-1, 1]$\n",
    "\n",
    "## solution: forget gates\n",
    "to address the issue of indefinite growth of cell states, the \"forget gate\" is introduced. the forget gate activation $y'_j$ is calculated similarly to other gates and is squashed using a logistic sigmoid function:\n",
    "\n",
    "$$\n",
    "y'_j (t) = f'_j \\left( \\sum_m w'_{jm} y_m(t-1) \\right)\n",
    "$$\n",
    "\n",
    "the revised update equation for the cell state $s_c$ in the extended lstm is:\n",
    "\n",
    "$$\n",
    "s_{cvj}(t) = y'_{j}(t) \\cdot s_{cvj}(t-1) + y_{in}(t) \\cdot g(\\text{net}_{cvj}(t))\n",
    "$$\n",
    "\n",
    "forget gates learn to reset the memory block when its contents are no longer useful, thereby preventing unbounded growth of internal states.\n",
    "\n",
    "## experiments\n",
    "\n",
    "to test the effectiveness of forget gates, the authors extended the embedded reber grammar (erg) problem to create a continual version, where the network must handle concatenated sequences without explicit resets. the results show that extended lstm with forget gates can solve the task more efficiently than standard lstm, especially when combined with learning rate decay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
