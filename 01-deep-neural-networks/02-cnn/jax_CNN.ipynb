{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXG7wxej9XcQ"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqWFqUAuXF3k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils import data\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from jax.tree_util import tree_map\n",
        "from flax.training import train_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMoGg28C8AHs"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP5J9SAfhrfy"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 128\n",
        "DATA_MEANS = np.array([0.49139968, 0.48215841, 0.44653091])\n",
        "DATA_STD = np.array([0.24703223, 0.24348513, 0.26158784])\n",
        "CROP_SCALES = (0.8, 1.0)\n",
        "CROP_RATIO = (0.9, 1.1)\n",
        "SEED = 42\n",
        "\n",
        "plt.style.use('dark_background')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViZX-LiJB4Rp"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E9pdbO4B6wu"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  \"\"\"A simple CNN model.\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = x.reshape((x.shape[0], -1))  # flatten\n",
        "    x = nn.Dense(features=256)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(features=10)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPb8kfJD-o61"
      },
      "source": [
        "## Load CIFAR10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCtFreSN7SKF"
      },
      "outputs": [],
      "source": [
        "# A helper function that normalizes the images between the values specified by the hyper-parameters.\n",
        "def image_to_numpy(img):\n",
        "  img = np.array(img, dtype=np.float32)\n",
        "  img = (img / 255. - DATA_MEANS) / DATA_STD\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5Lv_14B7TjR"
      },
      "outputs": [],
      "source": [
        "# A helper function that converts batches into numpy arrays instead of the default option which is torch tensors\n",
        "def numpy_collate(batch):\n",
        "  if isinstance(batch[0], np.ndarray):\n",
        "    return np.stack(batch)\n",
        "  elif isinstance(batch[0], (tuple, list)):\n",
        "    transposed = zip(*batch)\n",
        "    return [numpy_collate(samples) for samples in transposed]\n",
        "  else:\n",
        "    return np.array(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELraMkbB7me7",
        "outputId": "638c39a1-961f-4de5-d3c6-d34e6a8f1323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# images in the test set will only be converted into numpy arrays\n",
        "test_transform = image_to_numpy\n",
        "# images in the train set will be randomly flipped, cropped, and then converted to numpy arrays\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=CROP_SCALES, ratio=CROP_RATIO),\n",
        "    image_to_numpy\n",
        "])\n",
        "\n",
        "# Validation set should not use train_transform.\n",
        "train_dataset = torchvision.datasets.CIFAR10('data', train=True, transform=train_transform, download=True)\n",
        "val_dataset = torchvision.datasets.CIFAR10('data', train=True, transform=test_transform, download=True)\n",
        "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(SEED))\n",
        "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(SEED))\n",
        "test_set = torchvision.datasets.CIFAR10('data', train=False, transform=test_transform, download=True)\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2, persistent_workers=True, collate_fn=numpy_collate,\n",
        ")\n",
        "val_data_loader = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, persistent_workers=True, collate_fn=numpy_collate,\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, persistent_workers=True, collate_fn=numpy_collate,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi5N4LpzjIqH",
        "outputId": "00d9e925-024c-4800-e358-426a9ed527b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CHECK CHECK CHECK\n",
            "number of samples in train_set:         45000\n",
            "number of batches in train_data_loader: 351\n",
            "number of samples / batch size:         45000 / 128 = 351.5625\n",
            "number of samples in test_set:          10000\n",
            "number of batches in test_data_loader:  79\n",
            "number of samples / batch size:         10000 / 128 = 78.125\n"
          ]
        }
      ],
      "source": [
        "print('CHECK CHECK CHECK')\n",
        "print(f'number of samples in train_set:         {len(train_set)}')\n",
        "print(f'number of batches in train_data_loader: {len(train_data_loader)}')\n",
        "print(f'number of samples / batch size:         {len(train_set)} / {BATCH_SIZE} = {len(train_set)/BATCH_SIZE}')\n",
        "print(f'number of samples in test_set:          {len(test_set)}')\n",
        "print(f'number of batches in test_data_loader:  {len(test_data_loader)}')\n",
        "print(f'number of samples / batch size:         {len(test_set)} / {BATCH_SIZE} = {len(test_set)/BATCH_SIZE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69K-JNv5iAW2",
        "outputId": "76643d8c-e436-4817-c7ba-81ab79fb4605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of images in the first train batch: (128, 32, 32, 3)\n",
            "type of images in the first train batch: float64\n",
            "size of labels in the first train batch: (128,)\n",
            "type of labels in the first train batch: int64\n"
          ]
        }
      ],
      "source": [
        "print(f'size of images in the first train batch: {next(iter(train_data_loader))[0].shape}')\n",
        "print(f'type of images in the first train batch: {next(iter(train_data_loader))[0].dtype}')\n",
        "print(f'size of labels in the first train batch: {next(iter(train_data_loader))[1].shape}')\n",
        "print(f'type of labels in the first train batch: {next(iter(train_data_loader))[1].dtype}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRdstSEqAP3d"
      },
      "source": [
        "## Initializing The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgWiTp2JlfOP"
      },
      "outputs": [],
      "source": [
        "model = CNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3bOcuxbnYTS"
      },
      "outputs": [],
      "source": [
        "optimizer = optax.adam(learning_rate=1e-4)\n",
        "\n",
        "rng, inp_rng, init_rng = jax.random.split(jax.random.PRNGKey(SEED), 3)\n",
        "params = model.init(jax.random.PRNGKey(SEED),\n",
        "                    jax.random.normal(inp_rng, (BATCH_SIZE, 32, 32, 3)))\n",
        "\n",
        "model_state = train_state.TrainState.create(apply_fn=model.apply,\n",
        "                                            params=params,\n",
        "                                            tx=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-jK_nLAAXqj"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4thSEkwDAmY"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def apply_model(state, images, labels):\n",
        "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
        "\n",
        "  def loss_fn(params):\n",
        "    logits = state.apply_fn(params, images)\n",
        "    one_hot = jax.nn.one_hot(labels, logits.shape[1])\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
        "    return loss, logits\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "  return grads, loss, accuracy\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update_model(state, grads):\n",
        "  return state.apply_gradients(grads=grads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L94RsGADkQb"
      },
      "outputs": [],
      "source": [
        "def train_epoch(state, data_loader):\n",
        "  \"\"\"Train for a single epoch.\"\"\"\n",
        "\n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "\n",
        "  for batch in data_loader:\n",
        "    batch_images, batch_labels = batch\n",
        "    grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
        "    state = update_model(state, grads)\n",
        "    epoch_loss.append(loss)\n",
        "    epoch_accuracy.append(accuracy)\n",
        "  train_loss = np.mean(epoch_loss)\n",
        "  train_accuracy = np.mean(epoch_accuracy)\n",
        "  return state, train_loss, train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU6Urm5uebtX"
      },
      "outputs": [],
      "source": [
        "def train_model(state, train_data_loader, num_epochs):\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    state, train_loss, train_accuracy = train_epoch(state, train_data_loader)\n",
        "    print(f'epoch: {epoch:03d}, train loss: {train_loss:.4f}, train accuracy: {train_accuracy:.4f}')\n",
        "  return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "TyAfCqGjegGz",
        "outputId": "be342fe9-ffd0-4be4-c437-57f917ee352b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 000, train loss: 1.6864, train accuracy: 0.4053\n",
            "epoch: 001, train loss: 1.4299, train accuracy: 0.4950\n",
            "epoch: 002, train loss: 1.3411, train accuracy: 0.5284\n",
            "epoch: 003, train loss: 1.2739, train accuracy: 0.5558\n",
            "epoch: 004, train loss: 1.2233, train accuracy: 0.5709\n",
            "epoch: 005, train loss: 1.1863, train accuracy: 0.5870\n",
            "epoch: 006, train loss: 1.1503, train accuracy: 0.5981\n",
            "epoch: 007, train loss: 1.1159, train accuracy: 0.6104\n",
            "epoch: 008, train loss: 1.0908, train accuracy: 0.6219\n",
            "epoch: 009, train loss: 1.0639, train accuracy: 0.6301\n",
            "epoch: 010, train loss: 1.0373, train accuracy: 0.6405\n",
            "epoch: 011, train loss: 1.0192, train accuracy: 0.6453\n",
            "epoch: 012, train loss: 0.9989, train accuracy: 0.6539\n",
            "epoch: 013, train loss: 0.9805, train accuracy: 0.6603\n",
            "epoch: 014, train loss: 0.9629, train accuracy: 0.6667\n",
            "epoch: 015, train loss: 0.9467, train accuracy: 0.6701\n",
            "epoch: 016, train loss: 0.9314, train accuracy: 0.6780\n",
            "epoch: 017, train loss: 0.9152, train accuracy: 0.6816\n",
            "epoch: 018, train loss: 0.9029, train accuracy: 0.6872\n",
            "epoch: 019, train loss: 0.8907, train accuracy: 0.6918\n",
            "epoch: 020, train loss: 0.8742, train accuracy: 0.6982\n",
            "epoch: 021, train loss: 0.8618, train accuracy: 0.7001\n",
            "epoch: 022, train loss: 0.8478, train accuracy: 0.7072\n",
            "epoch: 023, train loss: 0.8406, train accuracy: 0.7098\n",
            "epoch: 024, train loss: 0.8239, train accuracy: 0.7149\n",
            "epoch: 025, train loss: 0.8157, train accuracy: 0.7176\n",
            "epoch: 026, train loss: 0.8039, train accuracy: 0.7238\n",
            "epoch: 027, train loss: 0.7948, train accuracy: 0.7253\n",
            "epoch: 028, train loss: 0.7814, train accuracy: 0.7292\n",
            "epoch: 029, train loss: 0.7747, train accuracy: 0.7337\n",
            "epoch: 030, train loss: 0.7628, train accuracy: 0.7354\n",
            "epoch: 031, train loss: 0.7526, train accuracy: 0.7406\n",
            "epoch: 032, train loss: 0.7412, train accuracy: 0.7448\n",
            "epoch: 033, train loss: 0.7316, train accuracy: 0.7490\n",
            "epoch: 034, train loss: 0.7255, train accuracy: 0.7506\n",
            "epoch: 035, train loss: 0.7135, train accuracy: 0.7535\n",
            "epoch: 036, train loss: 0.7088, train accuracy: 0.7557\n",
            "epoch: 037, train loss: 0.6990, train accuracy: 0.7607\n",
            "epoch: 038, train loss: 0.6924, train accuracy: 0.7618\n",
            "epoch: 039, train loss: 0.6827, train accuracy: 0.7648\n",
            "epoch: 040, train loss: 0.6737, train accuracy: 0.7681\n",
            "epoch: 041, train loss: 0.6693, train accuracy: 0.7680\n",
            "epoch: 042, train loss: 0.6604, train accuracy: 0.7722\n",
            "epoch: 043, train loss: 0.6528, train accuracy: 0.7747\n",
            "epoch: 044, train loss: 0.6435, train accuracy: 0.7766\n",
            "epoch: 045, train loss: 0.6394, train accuracy: 0.7801\n",
            "epoch: 046, train loss: 0.6283, train accuracy: 0.7844\n",
            "epoch: 047, train loss: 0.6212, train accuracy: 0.7875\n",
            "epoch: 048, train loss: 0.6212, train accuracy: 0.7863\n",
            "epoch: 049, train loss: 0.6122, train accuracy: 0.7898\n",
            "epoch: 050, train loss: 0.5993, train accuracy: 0.7942\n",
            "epoch: 051, train loss: 0.5956, train accuracy: 0.7984\n",
            "epoch: 052, train loss: 0.5873, train accuracy: 0.7973\n",
            "epoch: 053, train loss: 0.5811, train accuracy: 0.7992\n",
            "epoch: 054, train loss: 0.5766, train accuracy: 0.8021\n",
            "epoch: 055, train loss: 0.5661, train accuracy: 0.8050\n",
            "epoch: 056, train loss: 0.5643, train accuracy: 0.8079\n",
            "epoch: 057, train loss: 0.5525, train accuracy: 0.8100\n",
            "epoch: 058, train loss: 0.5471, train accuracy: 0.8135\n",
            "epoch: 059, train loss: 0.5441, train accuracy: 0.8126\n",
            "epoch: 060, train loss: 0.5369, train accuracy: 0.8140\n",
            "epoch: 061, train loss: 0.5344, train accuracy: 0.8159\n",
            "epoch: 062, train loss: 0.5245, train accuracy: 0.8204\n",
            "epoch: 063, train loss: 0.5196, train accuracy: 0.8214\n",
            "epoch: 064, train loss: 0.5174, train accuracy: 0.8227\n",
            "epoch: 065, train loss: 0.5090, train accuracy: 0.8262\n",
            "epoch: 066, train loss: 0.5021, train accuracy: 0.8281\n",
            "epoch: 067, train loss: 0.4953, train accuracy: 0.8304\n",
            "epoch: 068, train loss: 0.4918, train accuracy: 0.8320\n",
            "epoch: 069, train loss: 0.4871, train accuracy: 0.8344\n",
            "epoch: 070, train loss: 0.4830, train accuracy: 0.8344\n",
            "epoch: 071, train loss: 0.4725, train accuracy: 0.8376\n",
            "epoch: 072, train loss: 0.4639, train accuracy: 0.8415\n",
            "epoch: 073, train loss: 0.4659, train accuracy: 0.8403\n",
            "epoch: 074, train loss: 0.4589, train accuracy: 0.8440\n",
            "epoch: 075, train loss: 0.4549, train accuracy: 0.8450\n",
            "epoch: 076, train loss: 0.4458, train accuracy: 0.8461\n",
            "epoch: 077, train loss: 0.4408, train accuracy: 0.8484\n",
            "epoch: 078, train loss: 0.4381, train accuracy: 0.8493\n",
            "epoch: 079, train loss: 0.4332, train accuracy: 0.8523\n",
            "epoch: 080, train loss: 0.4280, train accuracy: 0.8539\n",
            "epoch: 081, train loss: 0.4231, train accuracy: 0.8565\n",
            "epoch: 082, train loss: 0.4199, train accuracy: 0.8560\n",
            "epoch: 083, train loss: 0.4183, train accuracy: 0.8568\n",
            "epoch: 084, train loss: 0.4076, train accuracy: 0.8626\n",
            "epoch: 085, train loss: 0.4016, train accuracy: 0.8630\n",
            "epoch: 086, train loss: 0.4031, train accuracy: 0.8631\n",
            "epoch: 087, train loss: 0.3957, train accuracy: 0.8642\n",
            "epoch: 088, train loss: 0.3921, train accuracy: 0.8653\n",
            "epoch: 089, train loss: 0.3886, train accuracy: 0.8669\n",
            "epoch: 090, train loss: 0.3827, train accuracy: 0.8683\n",
            "epoch: 091, train loss: 0.3743, train accuracy: 0.8731\n",
            "epoch: 092, train loss: 0.3735, train accuracy: 0.8740\n",
            "epoch: 093, train loss: 0.3745, train accuracy: 0.8728\n",
            "epoch: 094, train loss: 0.3649, train accuracy: 0.8778\n",
            "epoch: 095, train loss: 0.3631, train accuracy: 0.8756\n",
            "epoch: 096, train loss: 0.3547, train accuracy: 0.8796\n",
            "epoch: 097, train loss: 0.3566, train accuracy: 0.8782\n",
            "epoch: 098, train loss: 0.3503, train accuracy: 0.8816\n",
            "epoch: 099, train loss: 0.3458, train accuracy: 0.8837\n"
          ]
        }
      ],
      "source": [
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "trained_model_state = train_model(model_state, train_data_loader, num_epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57aUBzldAdXc"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zPeCn_UL3-o",
        "outputId": "956a5f08-71ef-4d8d-cc11-2bd418fdd74f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.9002, accuracy: 0.7315\n"
          ]
        }
      ],
      "source": [
        "test_loss = []\n",
        "test_accuracy = []\n",
        "\n",
        "for batch in test_data_loader:\n",
        "  batch_images, batch_labels = batch\n",
        "  _, loss, accuracy = apply_model(trained_model_state, batch_images, batch_labels)\n",
        "  test_loss.append(loss)\n",
        "  test_accuracy.append(accuracy)\n",
        "\n",
        "print(f'loss: {np.mean(test_loss):.4f}, accuracy: {np.mean(test_accuracy):.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
