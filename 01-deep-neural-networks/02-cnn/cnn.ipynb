{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The CNN And Learning Almost Everything About It\n",
    "\n",
    "#### 1. Why CNN and Not MLP For Images?\n",
    "#### 2. How CNN Works? - Layers: Convolutional, Pooling, and Fully Connected Layers\n",
    "#### 3. Implementing Conv2d From Scratch\n",
    "#### 4. Putting It All Together With PyTorch\n",
    "\n",
    "![CNN](../../images/cnn.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Why We Use The CNN And Not MLP For Images?\n",
    "\n",
    "CNNs handle image datasets better than MLPs for two main reasons: **Sparse Connectivity** and **Parameter Sharing**.\n",
    "\n",
    "**Sparse Connectivity:** \n",
    "Sparse connectivity means that each neuron in a convolutional layer of a CNN is only connected to a small, localized region of the input image, rather than being fully connected to the entire input. This localized region is known as the receptive field.\n",
    "\n",
    "**Local Receptive Fields:** \n",
    "In CNNs, each convolutional kernel operates over a small patch of the input image. This means that neurons in the convolutional layer only focus on small regions of the input, making the network more efficient by reducing the number of parameters and computations.\n",
    "\n",
    "**Parameter Sharing:** \n",
    "The same convolutional filter is applied to the entire input image. This parameter sharing allows CNNs to capture features like edges, textures, and patterns regardless of their position in the image.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Not MLP?\n",
    "\n",
    "**High Number of Parameters:** \n",
    "For an image of size 32x32 with 3 color channels, a single neuron in the first hidden layer of an MLP would have \\(32 \\times 32 \\times 3 = 3072\\) connections. With a large number of neurons, this quickly becomes impractical.\n",
    "\n",
    "**Lack of Locality:** \n",
    "MLPs treat all input pixels equally without considering the spatial structure of the image. This lack of locality makes it harder for MLPs to capture local patterns and structures that are important for image recognition tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How CNN Works?\n",
    "\n",
    "#### Layers: Convolutional, Pooling, and Fully Connected Layers\n",
    "\n",
    "The CNN model primarily consists of a stack of layers: convolutional and pooling layers, followed by fully connected layers (MLP). Depending on the specific problem, the output layer is generally either sigmoid or softmax.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Convolutional Layer \n",
    "\n",
    "*(In the original convolution formula, the filter \\( W \\) (parameters) is rotated. However, in libraries like PyTorch, \\( W \\) is used as is. This operation is technically called \"cross-correlation.\")*\n",
    "\n",
    "![Convolution](../../images/convolution-2.gif)\n",
    "\n",
    "The image first passes through the convolutional layer, which uses a kernel (filled with weights) that slides over the entire image, as shown above. This process extracts features such as edges, textures, and patterns. Modern CNNs usually use kernel sizes such as 1×1, 3×3, or 5×5.\n",
    "\n",
    "##### 2D Convolution Formula:\n",
    "\n",
    "The 2D convolution formula can be expressed as:\n",
    "\n",
    "$$\n",
    "Y = X * W \\implies Y[i, j] = \\sum_m \\sum_n X[i - m, j - n] W[m, n]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the input image matrix.\n",
    "- \\( W \\) is the kernel (filter) matrix.\n",
    "- \\( Y \\) is the output matrix.\n",
    "- \\( i \\) and \\( j \\) are the indices of the output matrix.\n",
    "- \\( m \\) and \\( n \\) are the indices of the kernel matrix.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Pooling Layer\n",
    "\n",
    "![Pooling](../../images/maxpool.gif)\n",
    "\n",
    "After the convolutional layer, the output is passed to the pooling layer. Pooling reduces the spatial dimensions of the feature maps, which helps decrease the computational load and the number of parameters in the network. It also makes the network invariant to small translations of the input image. \n",
    "\n",
    "There are two common types of pooling:\n",
    "- **MaxPooling**, which selects the maximum value from each patch of the feature map.\n",
    "- **AveragePooling**, which computes the average value of each patch.\n",
    "\n",
    "This layer helps summarize the presence of features in patches of the feature map, rather than their precise location.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Fully Connected Layer\n",
    "\n",
    "The output from the stack of convolutional and pooling layers is then flattened into a one-dimensional vector and passed to the fully connected layer (MLP). This layer performs high-level reasoning by combining all the features detected in previous layers. Each neuron in a fully connected layer is connected to every neuron in the previous layer, allowing the network to learn complex representations and relationships between features.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Output Layer\n",
    "\n",
    "Finally, the output from the fully connected layer is passed to the output layer. The type of output layer depends on the nature of the problem. For a binary classification problem, a sigmoid activation function is often used, which outputs a probability between 0 and 1. For multiclass classification problems, a softmax activation function is typically used, which outputs a probability distribution over all classes, allowing the network to predict the class with the highest probability.\n",
    "\n",
    "---\n",
    "\n",
    "This layered approach allows CNNs to effectively process and classify images by learning and focusing on important features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementing Convo2d From Scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our_conv2d: \n",
      "[[ -7.  -4.   7.   9.   0.]\n",
      " [-15.  -6.  15.  18.   0.]\n",
      " [-13.  -4.  13.  15.   0.]\n",
      " [ -8.  -2.   8.   9.   0.]\n",
      " [  0.   0.   0.   0.   0.]]\n",
      "PyTorch conv2d output:\n",
      "[[ -7.  -4.   7.   9.   0.]\n",
      " [-15.  -6.  15.  18.   0.]\n",
      " [-13.  -4.  13.  15.   0.]\n",
      " [ -8.  -2.   8.   9.   0.]\n",
      " [  0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv2d(input, kernel, padding=0,stride=1):\n",
    "    # Extracting dimensions of the input and kernel\n",
    "    input_height, input_width = input.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    # Calculate output dimensions\n",
    "    output_height = (input_height - kernel_height + 2 * padding // stride) + 1\n",
    "    output_width = (input_width - kernel_width + 2 * padding // stride) + 1\n",
    "\n",
    "    # Apply padding to the input if needed\n",
    "    if padding > 0:\n",
    "        padded_input = np.zeros((input_height + 2 * padding, input_width + 2 * padding)) #adding from the both side \n",
    "        # print(padded_input) \n",
    "        padded_input[padding:padding + input_height, padding:padding + input_width] = input #adding to the input \n",
    "    else:\n",
    "        padded_input = input\n",
    "\n",
    "    # Initialize the output\n",
    "    output = np.zeros((output_height, output_width))\n",
    "\n",
    "    # Perform the convolution operation\n",
    "    for y in range(output_height):\n",
    "        for x in range(output_width):\n",
    "            region = padded_input[y:y + kernel_height, x:x + kernel_width]\n",
    "            output[y, x] = np.sum(region * kernel)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "input_matrix = np.array([\n",
    "    [1, 2, 3, 0, 0],\n",
    "    [4, 5, 6, 0, 0],\n",
    "    [7, 8, 9, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "kernel_matrix = np.array([\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "])\n",
    "\n",
    "output_matrix = conv2d(input_matrix, kernel_matrix, padding=1,stride = 1)\n",
    "print(\"our_conv2d: \")\n",
    "print(output_matrix)\n",
    "\n",
    "#---------------------------------------PyTorch----------------------------------------#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input and kernel\n",
    "input_tensor = torch.tensor(input_matrix, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  \n",
    "kernel_tensor = torch.tensor(kernel_matrix, dtype=torch.float32).unsqueeze(0).unsqueeze(0) \n",
    "\n",
    "# Define the conv2d operation in PyTorch\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, stride=1, bias=False)\n",
    "conv.weight.data = kernel_tensor\n",
    "\n",
    "# Perform the convolution\n",
    "output_tensor = conv(input_tensor)\n",
    "output_pytorch = output_tensor.squeeze().detach().numpy()\n",
    "\n",
    "print(\"PyTorch conv2d output:\")\n",
    "print(output_pytorch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Putting It All Together With PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#lets get the data \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.gelu(self.fc1(x))  # using gelu instead of gelu (idk why)\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.815\n",
      "[1,  4000] loss: 1.537\n",
      "[1,  6000] loss: 1.447\n",
      "[1,  8000] loss: 1.410\n",
      "[1, 10000] loss: 1.375\n",
      "[1, 12000] loss: 1.324\n",
      "[2,  2000] loss: 1.237\n",
      "[2,  4000] loss: 1.240\n",
      "[2,  6000] loss: 1.240\n",
      "[2,  8000] loss: 1.226\n",
      "[2, 10000] loss: 1.219\n",
      "[2, 12000] loss: 1.193\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
