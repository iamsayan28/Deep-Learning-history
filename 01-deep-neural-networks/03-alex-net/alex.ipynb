{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexnet\n",
    "\n",
    "## Before AlexNet: The Pre-Deep Neural Network Era\n",
    "\n",
    "- Researchers used to trained machine learning models on CPUs.\n",
    "\n",
    "- CPUs had limited capacity so were not able to train the large models.\n",
    "- Training with large datasets was challenging.\n",
    "- LeNet was one of the first models trained on medium-sized datasets, but not truly large ones.\n",
    "- Hardware limitations were a major factor in using small datasets with fewer parameters:\n",
    "  - NVIDIA's GeForce 256 from 1999 could process at most 480 million floating-point operations.\n",
    "  \n",
    "  - There were no meaningful programming frameworks like CUDA to operate these accelerators.\n",
    "  - In contrast, today's accelerators can perform over 1000 TFLOPs per device.\n",
    "- Activation functions were not as sophisticated or effective.\n",
    "- Moreover, datasets were still relatively small: OCR on 60,000 low-resolution 28 X 28 pixel images was considered a highly challenging task.\n",
    "\n",
    "## ImageNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
