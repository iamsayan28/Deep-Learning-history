{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before AlexNet: The Pre-Deep Neural Network ERA\n",
    "\n",
    "---\n",
    "\n",
    "- Researchers used to trained machine learning models on CPUs.\n",
    "\n",
    "- CPUs had limited capacity so were not able to train the large models.\n",
    "- Training with large datasets was challenging.\n",
    "- LeNet was one of the first models trained on medium-sized datasets, but not truly large ones.\n",
    "- Hardware limitations were a major factor in using small datasets with fewer parameters:\n",
    "  - NVIDIA's GeForce 256 from 1999 could process at most 480 million floating-point operations.\n",
    "  \n",
    "  - There were no meaningful programming frameworks like CUDA to operate these accelerators.\n",
    "  - In contrast, today's accelerators can perform over 1000 TFLOPs per device.\n",
    "- Activation functions were not as effective.\n",
    "- Moreover, datasets were still relatively small: OCR on 60,000 low-resolution 28 X 28 pixel images was considered a highly challenging task.\n",
    "---\n",
    "## ImageNet \n",
    "\n",
    "-  ImageNet was released in 2009, the dataset comprised 12 million images across 22,000 categories.\n",
    "\n",
    "-------\n",
    "\n",
    "![imagnet](../../images/imagent.webp)\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "- The team which created the ImageNet Datasets started organising the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\n",
    "\n",
    "- The team with lowest error rate will win.\n",
    "- The ImageNet Challenge becomes popular among the reasechers and become the standard to evaluate the performance of the vision models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet: The Beginning of the Deep Neural Network ERA\n",
    "\n",
    "---\n",
    "\n",
    "- AlexNet had 60 million parameters.\n",
    "\n",
    "- Training on CPUs was impractical due to the large number of parameters.\n",
    "- A major breakthrough occurred when Alex Krizhevsky and Ilya Sutskever implemented a deep CNN that could run on GPUs.\n",
    "- They realized that the computational bottlenecks in CNNs, such as convolutions and matrix multiplications, could be parallelized in hardware.\n",
    "- Using two NVIDIA GTX 580s with 3GB of memory, each capable of 1.5 TFLOPs, they implemented fast convolutions. Training one model on a single GPU was not possible at the time.\n",
    "- The two halves of the network would communicate at specific layers to ensure they were not training two separate models.\n",
    "- AlexNet was released in 2012 and won the ImageNet competition by a large margin in error rate.\n",
    "- The authors introduced numerous methods to improve the performance of AlexNet.\n",
    "- This paper completely changed the AI field.\n",
    "\n",
    "---\n",
    "- ![imgnet](../../images/imagnet-win.webp) \n",
    "----\n",
    "## Architecture\n",
    "\n",
    "**The network consists of 8 learned layers:**\n",
    "- 5 convolutional layers\n",
    "- 3 fully-connected layers\n",
    "---\n",
    "![arc](../../images/alexnet-arc.webp)\n",
    "\n",
    "---\n",
    "### ReLU Nonlinearity\n",
    "\n",
    "- Used f(x) = max(0, x) as the activation function.\n",
    "\n",
    "- Trains several times faster than tanh units.\n",
    "- Does not require input normalization to prevent saturation.\n",
    "---\n",
    "- ![relu](../../images/relu.webp)\n",
    "\n",
    "\n",
    "### Local Response Normalization\n",
    "\n",
    "- Applied after the ReLU activation in certain layers (specifically after the first and second convolutional layers in AlexNet).\n",
    "\n",
    "- By normalizing the responses, it prevents a single feature from dominating.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Overlapping Pooling\n",
    "\n",
    "- Pooling layers summarize outputs of neighboring groups of neurons\n",
    "\n",
    "- Use overlapping pooling: z > s, where z is the filter size and s is the stride\n",
    "- Reduces top-1 and top-5 error rates rates by 0.4% and 0.3%. \n",
    "---\n",
    "- ![overlapping](../../images/overlapping.webp)\n",
    "\n",
    "---\n",
    "\n",
    "## Reducing Overfitting\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "Two forms of data augmentation were used:\n",
    "\n",
    "1. Image translations and horizontal reflections\n",
    "   - Extract random 224x224 patches (and their horizontal reflections) from 256x256 images\n",
    "   - Increases training set by a factor of 2048\n",
    "\n",
    "2. Altering RGB channel intensities\n",
    "   - Performs PCA on RGB pixel values in training set\n",
    "   - Adds multiples of principal components to each training image\n",
    "\n",
    "### Dropout\n",
    "\n",
    "- Randomly drops out neurons during training (probability 0.5)\n",
    "\n",
    "- Reduces complex co-adaptations of neurons\n",
    "- Forces the network to learn more robust features\n",
    "- Used in the first two fully-connected layers\n",
    "---\n",
    "- ![dorpout](../../images/dropout2.svg)\n",
    "\n",
    "--- \n",
    "\n",
    "## GPU Utilization \n",
    "\n",
    "- The network was trained using two NVIDIA GTX 580 GPUs\n",
    "\n",
    "- Each GPU responsible for roughly half of the neurons/kernels\n",
    "- One GPU handles top half of kernels/neurons and  Other GPU handles bottom half, Reduces training time significantly.\n",
    "\n",
    "## Training \n",
    "\n",
    "- Stochastic gradient descent with batch size of 128\n",
    "\n",
    "- Momentum of 0.9 and weight decay of 0.0005\n",
    "- Learning rate initialized at 0.01, reduced by factor of 10 when validation error rate stopped improving\n",
    "- Trained for approximately 90 epochs through the training set of 1.2 million images and 1000 classes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexnet In Work (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
