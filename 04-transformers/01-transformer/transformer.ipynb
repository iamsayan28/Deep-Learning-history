{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "transformers are a type of deep learning model architecture that has revolutionized natural language processing (NLP) and various other domains. Introduced in the 2017 paper \"Attention Is All You Need\" by Vaswani et al., transformers have become the foundation for many state-of-the-art models in NLP and beyond.\n",
    "\n",
    "imp components of tranformer : \n",
    "\n",
    "1. self-attention : allows the model to weigh the importance of different parts of the input when processing each element.\n",
    "\n",
    "2. parallelization: Unlike recurrent neural networks (RNNs), transformers can process all input elements simultaneously, leading to faster training and inference.\n",
    "\n",
    "3. positional encoding: it helps the model to understand the order of input elements without using recurrence.(there has been little update here like ROPE(rotary positional encoding))\n",
    "\n",
    "4. encoder-decoder architecture: suitable for various tasks, including machine translation, text summarization, and more.\n",
    "\n",
    "5. scalability: transformers can be scaled to very large models with billions of parameters, leading to impressive performance on many tasks.\n",
    "\n",
    "early SOTA model based on transfomer : \n",
    "- BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- GPT (Generative Pre-trained Transformer) series\n",
    "- T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "These models have achieved state-of-the-art results in various NLP tasks and have been adapted for use in other domains like computer vision and speech processing.(with some samll chnages in arch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
