{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's understand vision transformer (vit)\n",
    "\n",
    "> **why do we need vision transformer?**\n",
    "- traditionally, convolutional neural networks (cnns) dominated computer vision tasks.\n",
    "- cnns have built-in inductive biases for images, but these can sometimes limit what the model learns.\n",
    "- transformer architecture had revolutionized nlp, suggesting it might also benefit vision.\n",
    "- vision transformer brings the power of self-attention to image processing, enabling better global understanding of images.\n",
    "- vit can capture long-range dependencies between image patches that might be missed by cnns.\n",
    "\n",
    "> **what is a vision transformer?**\n",
    "- vision transformer (vit) adapts the transformer architecture from nlp to computer vision.\n",
    "- instead of processing text tokens, it processes image patches as tokens.\n",
    "- the key insight: split an image into fixed-size patches and treat each patch like a word token.\n",
    "- this approach removes the convolution operations entirely in the pure vit design.\n",
    "- it applies the same self-attention mechanism that made transformers successful in language tasks.\n",
    "\n",
    "> **how vision transformer works?**\n",
    "- 1. split the image into fixed-size patches (like 16Ã—16 pixels)\n",
    "- 2. flatten each patch into a 1d vector\n",
    "- 3. project these vectors to the model dimension\n",
    "- 4. add position embeddings to retain spatial information\n",
    "- 5. process through standard transformer encoder blocks\n",
    "- 6. use the output of the special [class] token for classification\n",
    "\n",
    "\n",
    "\n",
    "> **transformer encoder block:**\n",
    "- each block contains:\n",
    "  - multi-head self-attention (msa)\n",
    "  - layer normalization (ln)\n",
    "  - multilayer perceptron (mlp)\n",
    "  - residual connections\n",
    "\n",
    "\n",
    "\n",
    "> **key advantages of vit**\n",
    "- global receptive field from the start (unlike cnns which build this gradually)\n",
    "- flexible attention to relevant parts of the image regardless of distance\n",
    "- fewer inductive biases, allowing the model to learn more complex patterns\n",
    "- excellent scaling properties - performance improves predictably with more data and compute\n",
    "- ability to visualize attention maps to see what the model focuses on\n",
    "- transfer learning capabilities across different vision tasks\n",
    "\n",
    "> **challenges and solutions**\n",
    "- requires more data than cnns to reach similar performance\n",
    "- computationally intensive for high-resolution images\n",
    "- positional information must be explicitly added\n",
    "- hybrid approaches combining cnn features with transformers often work best in practice\n",
    "- data augmentation and regularization are crucial for good performance\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
