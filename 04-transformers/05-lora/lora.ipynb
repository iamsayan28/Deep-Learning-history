{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's understand lora (low-rank adaptation)\n",
    "\n",
    "> **why do we need lora?**\n",
    "- when we fine-tune large language models, it's incredibly expensive to update all parameters.\n",
    "- lora provides a memory-efficient alternative that achieves similar results while only training a small fraction of parameters.\n",
    "- traditional fine-tuning requires storing and updating the entire model, which is impractical for most users without expensive hardware.\n",
    "- lora introduces a clever \"bypass\" solution that keeps the original pre-trained weights frozen and only trains small adapter modules.\n",
    "\n",
    "> **what is lora?**\n",
    "- lora stands for low-rank adaptation, a technique that makes fine-tuning large models more accessible.\n",
    "- instead of modifying all weights directly, lora decomposes weight updates into smaller matrices through low-rank decomposition.\n",
    "- this dramatically reduces the number of trainable parameters (often by 10,000x or more) while maintaining performance.\n",
    "- example: instead of training billions of parameters in a large model, lora might only train a few million parameters.\n",
    "\n",
    "> **how lora works?**\n",
    "- lora freezes the pre-trained model weights completely.\n",
    "- for each weight matrix we want to adapt, lora adds a parallel \"bypass\" connection.\n",
    "- this bypass consists of two smaller matrices: a down-projection and an up-projection.\n",
    "- the original path: input → original frozen weight → output\n",
    "- the lora path: input → down-projection → up-projection → output\n",
    "- the final output combines both paths.\n",
    "\n",
    "> **three key steps**\n",
    "- 1. decompose each weight matrix update into two smaller matrices (down-projection and up-projection)\n",
    "- 2. initialize these matrices so their product is zero (ensuring no change to behavior initially)\n",
    "- 3. train only these small matrices while keeping the original weights frozen\n",
    "\n",
    "> **why is this efficient?**\n",
    "- the rank of these matrices (r) is tiny compared to the original dimensions.\n",
    "- this makes the number of trainable parameters much smaller than the original model.\n",
    "- storage requirements are reduced significantly, often enabling fine-tuning on consumer hardware.\n",
    "- during inference, lora matrices can be merged with the original weights with no performance penalty.\n",
    "\n",
    "> **benefits of lora**\n",
    "- dramatically reduced memory requirements for fine-tuning\n",
    "- faster training times\n",
    "- lower computational costs\n",
    "- ability to switch between different adaptations quickly\n",
    "- preserves the general knowledge of the base model while adding specialized capabilities"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
