{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks !!!\n",
    "\n",
    "> Why there is need of residual connection?\n",
    "\n",
    "- As the depth of CNN model increases, the performance keeps getting better.(till the certain layers)\n",
    "\n",
    "- Ex. VGG (18-layers) beats AlexNet (8-layers) performance.\n",
    "\n",
    "- But after increasing more layers in VGG, it starts decreasing its performance.\n",
    "\n",
    "- Residual connection helps in improving training for larger models.\n",
    "\n",
    "- Without losing its performance even after increasing its depth.\n",
    "\n",
    "- But using residual connection can cause an exponential increase in the magnitude of activation at initialization.\n",
    "\n",
    "- To control and re-center the magnitude of activation, it uses batch-norm at each layer (residual block).\n",
    "\n",
    "- **additional Note:** ResNets provide a direct path for the gradient, mitigating the problem of vanishing gradients, and allowing the network to train effectively even as it becomes deeper.\n",
    "\n",
    "> Why model with more depth start decreasing its performance?\n",
    "\n",
    "- So the model with no skip or residual connection is known as a sequential model.\n",
    "\n",
    "- Ex. layer = l1, l2, l3, and l4 -> model_with_ln -> y =  (l4(l3(l2(l1(x))))) (forward pass in seq model looks like this, right?)\n",
    "\n",
    "- In a deep neural network, even a small change in input can cause a lot of change in its gradient. This is known as the shattered gradient phenomenon.\n",
    "\n",
    "- The shattered gradient phenomenon causes the gradient to become increasingly complex and less smooth as it propagates through many layers.\n",
    "\n",
    "- Due to this, the gradient descent algorithm (GDA) doesn't perform well because GDA needs a smooth gradient curve to perform well.\n",
    "\n",
    "- Because the performance of GDA depends on before and after gradient (due to the chain rule).\n",
    "\n",
    "- Check out the backprop file to understand this more.\n",
    "\n",
    "> Residual connections\n",
    "\n",
    "- We can add residuals followed by the linear transformation layer and activation function.\n",
    "\n",
    "- In practice, we generally add it after several layers of the network.\n",
    "\n",
    "- If we start res-block with ReLU, they will do nothing if input is zero (ReLU -> 0, neg).\n",
    "\n",
    "- Hence, we add the res-block after the linear transformation to avoid being lazy for negative values.\n",
    "\n",
    "- We cannot choose the depth of the network randomly even if it performs well because adding res-block will roughly double the depth of the network, which might affect the variance of activation during the forward pass, and as a result, cause gradient exploding for the backward pass.\n",
    "\n",
    "- The main purpose of residual connections is to allow the network to learn identity functions easily, effectively skipping layers that are not beneficial.\n",
    "\n",
    "> How to solve this variance problem that results in gradient exploding?\n",
    "\n",
    "- In order to handle the exponential variance problem, we need to normalize the inputs.\n",
    "\n",
    "- This can be solved with the help of batch-norm.\n",
    "\n",
    "> What is batch-norm?\n",
    "\n",
    "- To use the batch, we must provide the data in mini-batches to train.\n",
    "\n",
    "- As its name goes, it uses a batch to calculate the mean and std.\n",
    "\n",
    "- Mean and std are used as parameters to calculate batch-norm (these are running statistics, not learnable parameters).\n",
    "\n",
    "- Batch norm is very effective because it not only normalizes but also, if the model doesn't need normalizing, it shifts and scales the input using gamma and beta (learnable parameters).\n",
    "\n",
    "- **additional Note:** Batch normalization helps stabilize and accelerate the training process by normalizing the inputs of each layer. This is especially beneficial when combined with residual networks, further enhancing their ability to train deep architectures efficiently.\n",
    "\n",
    "- For more on batch-norm, check the link.\n",
    "\n",
    "> **Formula for ResNet:**\n",
    "\n",
    "- A basic residual block can be represented as:\n",
    "\n",
    "  \\( y = F(x, \\{W_i\\}) + x \\)\n",
    "\n",
    "  where \\( x \\) is the input, \\( F(x, \\{W_i\\}) \\) represents the residual function (e.g., a stack of convolutional layers), and \\( y \\) is the output.\n",
    "\n",
    "- The key idea is that instead of expecting each layer to directly learn a desired underlying mapping, it learns the residual mapping which is easier to optimize.\n",
    "\n",
    "- This formulation allows the network to easily learn identity mappings by pushing the residual to zero, which helps in training very deep networks.\n",
    "\n",
    "> **Internal Covariate Shift:**\n",
    "\n",
    "- Internal covariate shift refers to the change in the distribution of network activations due to the updating of network parameters during training.\n",
    "\n",
    "- Batch normalization helps to reduce this internal covariate shift by normalizing the output of each layer to have a mean of zero and a variance of one.\n",
    "\n",
    "- By normalizing the inputs of each layer, batch normalization ensures that the distribution of the activations remains consistent during training, which helps in stabilizing and speeding up the training process.\n",
    "\n",
    "- example : - if we train the model of image classification of rose or not rose but with only images of red roses and at time of testing we provide the images of roses with diff colour then model might not recosined it. (high level example)\n",
    "\n",
    "> additional Points from the Paper:\n",
    "\n",
    "- **Residual Block Configuration:** Residual blocks can be constructed with multiple layers, not just one. Typically, each residual block consists of two or three convolutional layers, and batch normalization is applied before each nonlinearity.\n",
    "\n",
    "- **Training Efficiency:** Residual networks, thanks to their architecture, are able to train much deeper networks without the problems of vanishing or exploding gradients. This results in more efficient training and better performance on deeper models.\n",
    "\n",
    "- **Performance:** results show that residual networks achieve lower error rates on benchmarks such as ImageNet.(resutling sota model)\n",
    "\n",
    "> uses \n",
    "\n",
    "- almost everywhere.(transformer, stable diffusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
