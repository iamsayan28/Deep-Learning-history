{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks !!!\n",
    "\n",
    "> Why there is need of residual connection ?\n",
    "\n",
    "- as the depth of cnn model increases the performace keeps getting better.\n",
    "\n",
    "- Ex. VGG(18-layers) beats Alexnet(8-layers) performace. \n",
    "\n",
    "- but after increaing more layer in VGG its start decreasing its performace.\n",
    "\n",
    "- residaul connection helps in improving training more larger models.\n",
    "\n",
    "- without loosing its performace even after increasing its depth.\n",
    "\n",
    "- but using residual connection can cuase exponential increase of magnitude of activation at initilization. \n",
    "\n",
    "- to control and re-centere the magnitude of actiation.\n",
    "\n",
    "- it uses batch-norm at each layer.(residual block)\n",
    "\n",
    "> Why model with more depth start decreasing its performance ? \n",
    "\n",
    "- so the model with no skip or residual connetion is known as seueantial model.\n",
    "\n",
    "- Ex. layer = l1, l2, l3 and l4 -> model_with_ln -> y =  (l4(l3(l2(l1(x))))) (forward pass in seq model look like this right ?)\n",
    "\n",
    "- in a deep neural network even a small change in a input can cuase lost of change in its gradient. (also konwn as shattered gradeint phenomenon)\n",
    "\n",
    "- due to this gradient descent algo(gda) doesn't perform well cuase gda needs smooth gradient curve to perform well.\n",
    "\n",
    "- cuase the performaance of gda is depend on before and after gradient.(due to chain rule)\n",
    "\n",
    "- check out the backprop file to understand this more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
