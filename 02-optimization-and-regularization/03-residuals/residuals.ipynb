{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks !!!\n",
    "\n",
    "> Why there is need of residual connection ?\n",
    "\n",
    "- as the depth of cnn model increases the performace keeps getting better.\n",
    "\n",
    "- Ex. VGG(18-layers) beats Alexnet(8-layers) performace. \n",
    "\n",
    "- but after increaing more layer in VGG its start decreasing its performace.\n",
    "\n",
    "- residaul connection helps in improving training more larger models.\n",
    "\n",
    "- without loosing its performace even after increasing its depth.\n",
    "\n",
    "- but using residual connection can cuase exponential increase of magnitude of activation at initilization. \n",
    "\n",
    "- to control and re-centere the magnitude of actiation.\n",
    "\n",
    "- it uses batch-norm at each layer.(residual block)\n",
    "\n",
    "> Why model with more depth start decreasing its performance ? \n",
    "\n",
    "- so the model with no skip or residual connetion is known as seueantial model.\n",
    "\n",
    "- Ex. layer = l1, l2, l3 and l4 -> model_with_ln -> y =  (l4(l3(l2(l1(x))))) (forward pass in seq model look like this right ?)\n",
    "\n",
    "- in a deep neural network even a small change in a input can cuase lost of change in its gradient. (also konwn as shattered gradeint phenomenon)\n",
    "\n",
    "- due to this gradient descent algo(gda) doesn't perform well cuase gda needs smooth gradient curve to perform well.\n",
    "\n",
    "- cuase the performaance of gda is depend on before and after gradient.(due to chain rule)\n",
    "\n",
    "- check out the backprop file to understand this more.\n",
    "\n",
    "> residual connections \n",
    "\n",
    "- we can add residual followed by the linear transformation layer and activation fn.\n",
    "\n",
    "- in practice we gernerally added it to after several layer of network.\n",
    "\n",
    "- if we start res-block with relu they will do nothing if input it zero. (relu->0,neg)\n",
    "\n",
    "- hence we add the res-block after the linear transformation to avoid being lazy for negative values.\n",
    "\n",
    "- we cannot choose the depth of network ranadomly even if it perform well, cuase adding res-block will roughly double the depth of network so it might affects on the variance of activation during for forward pass and in result for the gradient exploding for the backward pass.(problem ???)\n",
    "\n",
    "> how to solve this variace problem that result in gradient exploding ?\n",
    "\n",
    "- in order to handle the exp variance problem we need to nomralise it the inputs.\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "> what is batch-norm \n",
    "\n",
    "\n",
    "> \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
