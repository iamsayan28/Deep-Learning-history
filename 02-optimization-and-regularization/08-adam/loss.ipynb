{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function (will be talking in terms of probability) (working on this)\n",
    "\n",
    "> **What is a loss fucntion ?**\n",
    "\n",
    "- while training the model the loss fucntion is used to calculate the mismatch between actual vlaues and predicted values.\n",
    "\n",
    "- it requires to parameters to calulate it Y_hat(predicted) and y(actual value).\n",
    "\n",
    "- the goal is to find paramaters values that minimize the loss.\n",
    "\n",
    "> **Maximum Likelihood**\n",
    "\n",
    "-  here we compute the distribution over a outputs.(y_hat)\n",
    "\n",
    "- instead of guessing single outcomes the model predicts a range of possible outcomes and how likely each one is.\n",
    "\n",
    "- ex. if we guess someone's height it 6ft but we are guessing so it might be less or more. So we represent this using bell curve.\n",
    "\n",
    "  - to plot this we need mean and variance.\n",
    "\n",
    "> **Maximum Likelihood Criterion**\n",
    "\n",
    "- Maximum Likeihood Estimation : we want to get the right of values for our model's parameter in order to reduce the gap between our prediceted output and actual outputs.\n",
    "\n",
    "$$\n",
    "\\hat{\\phi} = \\argmax_{\\phi} \\left[ \\prod_{i=1}^{n} \\Pr(y_i \\mid x_i, \\phi) \\right]\n",
    "$$\n",
    "\n",
    "**Break Down of above equaton**\n",
    "\n",
    "1. **Model Prediction (Pr(y_i | x_i, φ)):**\n",
    "   - Pr(y_i | x_i, φ) represents the probability that our model predicts the actual outcome (y_i) given the input data (x_i) and the model parameters (φ).\n",
    "\n",
    "   - Think of this as the model saying, \"Based on my current parameters, there's an X% chance that this specific input (x_i) will result in this specific output (y_i).\"\n",
    "\n",
    "2. **Product of Probabilities:**\n",
    "   - ∏(i=1 to n) means we multiply these probabilities together for all the data points (from 1 to n).\n",
    "\n",
    "   - This gives us a combined probability that shows how likely it is that all our predictions match the actual data.\n",
    "\n",
    "3. **Maximizing the Combined Probability:**\n",
    "   - argmax_φ means we want to find the specific parameters (φ) that make this combined probability as large as possible.\n",
    "\n",
    "   - inshort, we're adjusting our model's paramters to [maximize] the chances that it predicts the actual outcomes correctly.\n",
    "\n",
    "> **Maximum log Likelihood**\n",
    "\n",
    "- MLC uses the product of the ouputs prob and it can genrate the very small values.\n",
    "\n",
    "- and aslo it is not a numericaly stable.\n",
    "\n",
    "**Example**\n",
    "\n",
    "If you flip a coin 10 times and each flip has a probability of 0.5 for heads, the product of probabilities for all heads is:\n",
    "\n",
    "$$\n",
    "0.5 \\times 0.5 \\times 0.5 \\times \\ldots = (0.5)^{10} = 0.0009765625\n",
    "$$\n",
    "\n",
    "This is a very small number, and if you had more flips, it would get even smaller.\n",
    "\n",
    "Taking the log of each probability and summing them:\n",
    "\n",
    "$$\n",
    "\\log(0.5) + \\log(0.5) + \\log(0.5) + \\ldots = 10 \\times \\log(0.5) \\approx 10 \\times (-0.693) = -6.93\n",
    "$$\n",
    "\n",
    "This sum is a manageable number and avoids the numerical issues of multiplying many small numbers.\n",
    "\n",
    "- This is why using log-liklihood is more practical where we take the sum and not product.\n",
    "\n",
    "> **Negative log Likelihood**\n",
    "\n",
    "- this reframed the problem as minimizes prolem.\n",
    "\n",
    "- most of optimizestion algorithms are the build to solve the minimizestion problem.\n",
    "\n",
    "- this changes our goal from finding the maximum value of the log-likelihood to finding the minimum value of the negative log-likelihood.\n",
    "\n",
    "\n",
    "- The negative log-likelihood is calculated as:\n",
    "\n",
    "$$\n",
    "-\\sum_{i=1}^{n} \\log \\Pr(y_i \\mid x_i, \\phi)\n",
    "$$\n",
    "\n",
    "- To find the parameter values (\\(\\phi\\)) that minimize the negative log-likelihood, we use:\n",
    "\n",
    "$$\n",
    "\\hat{\\phi} = \\argmin_{\\phi} \\left[ -\\sum_{i=1}^{n} \\log \\Pr(y_i \\mid x_i, \\phi) \\right]\n",
    "$$\n",
    "\n",
    "-  comapare to **Maximum log Likelihood** its now minizing problem with neg.\n",
    "\n",
    "- this is the final formula of **LOSS.**\n",
    "\n",
    "> **Inference**\n",
    "\n",
    "- instead of the network directly predicting a specific value (y), it now predicts a range of possible values with different likelihoods. This is called a probability distribution.\n",
    "\n",
    "- During inference, we need a single \"best guess\" answer. We choose the single value from the distribution that has the highest probability.\n",
    "\n",
    "- **finding the Best Guess:**\n",
    "- We use the argmax operation to find this best guess:\n",
    "$$\\hat{y} = \\arg\\max_y \\text{Pr}(y|f[x,\\hat{\\phi}])$$\n",
    "- This means \"find the value of $y$ that gives the highest probability, given our model's output.\"\n",
    "\n",
    "> **IN-short**\n",
    "\n",
    "- Log-Likelihood: A measure of model fit that we want to maximize.\n",
    "- Negative Log-Likelihood: The negative of the log-likelihood, which we want to minimize.\n",
    "   \n",
    "> **Recipe for constructing loss function**\n",
    "\n",
    "- This recipe outlines the process of creating loss functions for training probabilistic \n",
    "neural networks using the maximum likelihood approach.\n",
    "\n",
    "\n",
    "**1. Choose a Suitable Probability Distribution**\n",
    "\n",
    "$$\\text{Pr}(y|\\theta)$$\n",
    "\n",
    "- Choose a probability distribution that's appropriate for your prediction task.\n",
    "- This distribution is defined over the domain of the predictions $y$.\n",
    "$\\theta$ represents the parameters of this distribution.\n",
    "\n",
    "- Examples:\n",
    "\n",
    "- For regression tasks, you might choose a Normal (Gaussian) distribution.\n",
    "\n",
    "- For binary classification, you might choose a Bernoulli distribution.\n",
    "- For multi-class classification, you might choose a Categorical distribution.\n",
    "\n",
    "**2. Set the Machine Learning Model to Predict Distribution Parameters**\n",
    "\n",
    "$$\\theta = f[x, \\phi]$$\n",
    "$$\\text{Pr}(y|\\theta) = \\text{Pr}(y|f[x, \\phi])$$\n",
    "\n",
    "- Your neural network $f[x, \\phi]$ is set to predict the parameters $\\theta$ of the chosen distribution.\n",
    "$x$ is the input to the network.\n",
    "$\\phi$ represents the parameters of the neural network itself.\n",
    "\n",
    "- Example:\n",
    "\n",
    "- For a Normal distribution, the network might output the mean $\\mu$ and standard deviation $\\sigma$.\n",
    "\n",
    "**3. Train the Model by Minimizing Negative Log-Likelihood**\n",
    "$$\\hat{\\phi} = \\arg\\min_{\\phi} L[\\phi] = \\arg\\min_{\\phi} -\\sum_{i=1}^N \\log \\text{Pr}(y_i|f[x_i, \\phi])$$\n",
    "\n",
    "- We want to find the network parameters $\\hat{\\phi}$ that minimize the negative log-likelihood loss function.\n",
    "\n",
    "This is done over the entire training dataset of pairs ${x_i, y_i}$.\n",
    "$N$ is the number of training examples.\n",
    "\n",
    "- **Why negative log-likelihood?**\n",
    "\n",
    "- Using log transforms the product of probabilities into a sum, which is computationally easier to handle.\n",
    "\n",
    "- The negative sign turns the maximization problem into a minimization problem, \n",
    "which is conventionally used in optimization algorithms.\n",
    "\n",
    "**4. Perform Inference**\n",
    "\n",
    "- For a new test example $x$, you have two options:\n",
    "\n",
    "- Return the full distribution: $\\text{Pr}(y|f[x,\\hat{\\phi}])$\n",
    "\n",
    "- This gives you the complete probability distribution over possible outputs.\n",
    "\n",
    "\n",
    "- Return the maximum of this distribution:\n",
    "$$\\hat{y} = \\arg\\max_y \\text{Pr}(y|f[x,\\hat{\\phi}])$$\n",
    "\n",
    "- This gives you a single point estimate, which is often more practical for decision-making.\n",
    "\n",
    "\n",
    "> **IN-Short**\n",
    "\n",
    "- This approach allows the model to learn to predict not just a single value, but a full probability distribution over possible outputs.\n",
    "\n",
    "- By minimizing the negative log-likelihood, we're effectively maximizing the probability of the observed data given our model.\n",
    "- This method naturally handles uncertainty: the predicted distribution will be wider (more uncertain) for inputs the model is less confident about.\n",
    "- The choice of distribution in step 1 is crucial and should reflect the nature of your data and task.\n",
    "- This recipe forms the basis for many modern machine learning approaches.\n",
    "\n",
    "\n",
    "> **Ex. 1: Univariate regression**\n",
    "\n",
    "\n",
    "> **Ex. 2: Binary Classification**\n",
    "\n",
    "> **Ex. 3: Multiclass classification**\n",
    "\n",
    "> **Cross Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
