{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function (will be talking in terms of probability) (working on this)\n",
    "\n",
    "> **What is a loss fucntion ?**\n",
    "\n",
    "- while training the model the loss fucntion is used to calculate the mismatch between actual vlaues and predicted values.\n",
    "\n",
    "- it requires to parameters to calulate it Y_hat(predicted) and y(actual value).\n",
    "\n",
    "- the goal is to find paramaters values that minimize the loss.\n",
    "\n",
    "> **Maximum Likelihood**\n",
    "\n",
    "-  here we compute the distribution over a outputs.(y_hat)\n",
    "\n",
    "- instead of guessing single outcomes the model predicts a range of possible outcomes and how likely each one is.\n",
    "\n",
    "- ex. if we guess someone's height it 6ft but we are guessing so it might be less or more. So we represent this using bell curve.\n",
    "\n",
    "  - to plot this we need mean and variance.\n",
    "\n",
    "> **Maximum Likelihood Criterion**\n",
    "\n",
    "- Maximum Likeihood Estimation : we want to get the right of values for our model's parameter in order to reduce the gap between our prediceted output and actual outputs.\n",
    "\n",
    "$$\n",
    "\\hat{\\phi} = \\argmax_{\\phi} \\left[ \\prod_{i=1}^{n} \\Pr(y_i \\mid x_i, \\phi) \\right]\n",
    "$$\n",
    "\n",
    "**Break Down of above equaton**\n",
    "\n",
    "1. **Model Prediction (Pr(y_i | x_i, φ)):**\n",
    "   - Pr(y_i | x_i, φ) represents the probability that our model predicts the actual outcome (y_i) given the input data (x_i) and the model parameters (φ).\n",
    "\n",
    "   - Think of this as the model saying, \"Based on my current parameters, there's an X% chance that this specific input (x_i) will result in this specific output (y_i).\"\n",
    "\n",
    "2. **Product of Probabilities:**\n",
    "   - ∏(i=1 to n) means we multiply these probabilities together for all the data points (from 1 to n).\n",
    "\n",
    "   - This gives us a combined probability that shows how likely it is that all our predictions match the actual data.\n",
    "\n",
    "3. **Maximizing the Combined Probability:**\n",
    "   - argmax_φ means we want to find the specific parameters (φ) that make this combined probability as large as possible.\n",
    "\n",
    "   - inshort, we're adjusting our model's paramters to [maximize] the chances that it predicts the actual outcomes correctly.\n",
    "\n",
    "> **Maximum log Likelihood**\n",
    "\n",
    "- MLC uses the product of the ouputs prob and it can genrate the very small values.\n",
    "\n",
    "- and aslo it is not a numericaly stable.\n",
    "\n",
    "**Example**\n",
    "\n",
    "If you flip a coin 10 times and each flip has a probability of 0.5 for heads, the product of probabilities for all heads is:\n",
    "\n",
    "$$\n",
    "0.5 \\times 0.5 \\times 0.5 \\times \\ldots = (0.5)^{10} = 0.0009765625\n",
    "$$\n",
    "\n",
    "This is a very small number, and if you had more flips, it would get even smaller.\n",
    "\n",
    "Taking the log of each probability and summing them:\n",
    "\n",
    "$$\n",
    "\\log(0.5) + \\log(0.5) + \\log(0.5) + \\ldots = 10 \\times \\log(0.5) \\approx 10 \\times (-0.693) = -6.93\n",
    "$$\n",
    "\n",
    "This sum is a manageable number and avoids the numerical issues of multiplying many small numbers.\n",
    "\n",
    "- This is why using log-liklihood is more practical where we take the sum and not product.\n",
    "\n",
    "> **Negative log Likelihood**\n",
    "\n",
    "- this reframed the problem as minimizes prolem.\n",
    "\n",
    "- most of optimizestion algorithms are the build to solve the minimizestion problem.\n",
    "\n",
    "- this changes our goal from finding the maximum value of the log-likelihood to finding the minimum value of the negative log-likelihood.\n",
    "\n",
    "\n",
    "- The negative log-likelihood is calculated as:\n",
    "\n",
    "$$\n",
    "-\\sum_{i=1}^{n} \\log \\Pr(y_i \\mid x_i, \\phi)\n",
    "$$\n",
    "\n",
    "- To find the parameter values (\\(\\phi\\)) that minimize the negative log-likelihood, we use:\n",
    "\n",
    "$$\n",
    "\\hat{\\phi} = \\argmin_{\\phi} \\left[ -\\sum_{i=1}^{n} \\log \\Pr(y_i \\mid x_i, \\phi) \\right]\n",
    "$$\n",
    "\n",
    "-  comapare to **Maximum log Likelihood** its now minizing problem with neg.\n",
    "\n",
    "- this is the final formula of **LOSS.**\n",
    "\n",
    "> **Inference**\n",
    "\n",
    "- \n",
    "\n",
    "> **IN-short**\n",
    "\n",
    "- Log-Likelihood: A measure of model fit that we want to maximize.\n",
    "- Negative Log-Likelihood: The negative of the log-likelihood, which we want to minimize.\n",
    "   \n",
    "> **Recipe for constructing loss function**\n",
    "\n",
    "\n",
    "> **Ex. 1: Univariate regression**\n",
    "\n",
    "\n",
    "> **Ex. 2: Binary Classification**\n",
    "\n",
    "> **Ex. 3: Multiclass classification**\n",
    "\n",
    "> **Cross Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
