{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6735298284295481\n",
      "Epoch 100, Loss: 0.5499198684645213\n",
      "Epoch 200, Loss: 0.40330652791125765\n",
      "Epoch 300, Loss: 0.29090332772000466\n",
      "Epoch 400, Loss: 0.22158326212527774\n",
      "Epoch 500, Loss: 0.17868648371217677\n",
      "Epoch 600, Loss: 0.15047406159785276\n",
      "Epoch 700, Loss: 0.13076492939934675\n",
      "Epoch 800, Loss: 0.11629410532703004\n",
      "Epoch 900, Loss: 0.1052406971550421\n",
      "Final predictions: [[0.98977573]\n",
      " [0.99072822]\n",
      " [0.97251747]\n",
      " [0.96434438]\n",
      " [0.80288299]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def compute_loss(self, X, y, lambda_):\n",
    "        m = X.shape[0]\n",
    "        y_hat = self.forward(X)\n",
    "        loss = -np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) / m\n",
    "        \n",
    "        # Add L2 regularization term (weight decay)\n",
    "        l2_reg = (lambda_ / (2 * m)) * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2)))\n",
    "        return loss + l2_reg\n",
    "\n",
    "    def backward(self, X, y, learning_rate, lambda_):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Compute gradients\n",
    "        delta2 = self.a2 - y\n",
    "        dW2 = np.dot(self.a1.T, delta2) / m\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        delta1 = np.dot(delta2, self.W2.T) * self.sigmoid_derivative(self.a1)\n",
    "        dW1 = np.dot(X.T, delta1) / m\n",
    "        db1 = np.sum(delta1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Add weight decay term to weight gradients\n",
    "        dW2 += (lambda_ / m) * self.W2\n",
    "        dW1 += (lambda_ / m) * self.W1\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "def train_nn_with_weight_decay(X, y, hidden_size, epochs, learning_rate, lambda_):\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    \n",
    "    nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        nn.forward(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.compute_loss(X, y, lambda_)\n",
    "        \n",
    "        # Backward pass and update weights\n",
    "        nn.backward(X, y, learning_rate, lambda_)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    \n",
    "    return nn\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some example data\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(100, 2)\n",
    "    y = np.array([(x[0] + x[1] > 0).astype(int) for x in X]).reshape(-1, 1)\n",
    "\n",
    "    # Train the neural network\n",
    "    trained_nn = train_nn_with_weight_decay(X, y, hidden_size=4, epochs=1000, learning_rate=0.1, lambda_=0.01)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = trained_nn.forward(X)\n",
    "    print(\"Final predictions:\", predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabh/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/saurabh/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6934700012207031\n",
      "Epoch 100, Loss: 0.6093820929527283\n",
      "Epoch 200, Loss: 0.48696351051330566\n",
      "Epoch 300, Loss: 0.3752272427082062\n",
      "Epoch 400, Loss: 0.3016279637813568\n",
      "Epoch 500, Loss: 0.25658854842185974\n",
      "Epoch 600, Loss: 0.22852538526058197\n",
      "Epoch 700, Loss: 0.21040691435337067\n",
      "Epoch 800, Loss: 0.19831696152687073\n",
      "Epoch 900, Loss: 0.19003280997276306\n",
      "Final predictions: [[0.95439345]\n",
      " [0.9636781 ]\n",
      " [0.8900036 ]\n",
      " [0.8641708 ]\n",
      " [0.66619074]]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neural_network_weight_decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Compare with the custom implementation\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_network_weight_decay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_nn_with_weight_decay \u001b[38;5;28;01mas\u001b[39;00m custom_train\n\u001b[1;32m     66\u001b[0m custom_nn \u001b[38;5;241m=\u001b[39m custom_train(X, y, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     67\u001b[0m custom_predictions \u001b[38;5;241m=\u001b[39m custom_nn\u001b[38;5;241m.\u001b[39mforward(X)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neural_network_weight_decay'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.layer1(x))\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "def train_nn_with_weight_decay(X, y, hidden_size, epochs, learning_rate, lambda_):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X = torch.FloatTensor(X)\n",
    "    y = torch.FloatTensor(y)\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "\n",
    "    # Create the model\n",
    "    model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some example data (same as in the custom implementation)\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(100, 2)\n",
    "    y = np.array([(x[0] + x[1] > 0).astype(int) for x in X]).reshape(-1, 1)\n",
    "\n",
    "    # Train the neural network\n",
    "    trained_nn = train_nn_with_weight_decay(X, y, hidden_size=4, epochs=1000, learning_rate=0.1, lambda_=0.01)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = trained_nn(torch.FloatTensor(X))\n",
    "    print(\"Final predictions:\", predictions[:5].numpy())\n",
    "\n",
    "    # Compare with the custom implementation\n",
    "    import train_nn_with_weight_decay as custom_train\n",
    "\n",
    "    custom_nn = custom_train(X, y, hidden_size=4, epochs=1000, learning_rate=0.1, lambda_=0.01)\n",
    "    custom_predictions = custom_nn.forward(X)\n",
    "    print(\"Custom implementation predictions:\", custom_predictions[:5])\n",
    "\n",
    "    # Calculate mean squared error between PyTorch and custom predictions\n",
    "    mse = np.mean((predictions.numpy() - custom_predictions) ** 2)\n",
    "    print(f\"Mean Squared Error between PyTorch and custom predictions: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
