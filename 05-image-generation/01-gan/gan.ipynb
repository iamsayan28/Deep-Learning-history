{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's understand gan (generative adversarial networks)\n",
    "\n",
    "> **why do we need gan?**\n",
    "\n",
    "traditional generative models like variational autoencoders often produce blurry or unrealistic outputs. we needed a way to generate sharper, more realistic images, audio, and other media that truly captures the complexity of real-world data. supervised learning requires labeled data, but generative tasks often need to learn the underlying distribution of unlabeled data. gans introduced a revolutionary approach: instead of explicitly modeling probability distributions with mathematical formulas, they learn to generate data through an adversarial game between two neural networks.\n",
    "\n",
    "> **what is gan?**\n",
    "\n",
    "generative adversarial networks consist of two neural networks—a generator and a discriminator—that compete against each other in a minimax game. the generator creates fake samples trying to fool the discriminator, while the discriminator works to distinguish between real and fake samples. this adversarial process forces both networks to improve: the generator creates increasingly realistic data, and the discriminator becomes better at spotting subtle flaws. after training, the generator can create new, never-before-seen samples that closely resemble the training data distribution.\n",
    "\n",
    "> **how gan works?**\n",
    "\n",
    "the generator takes random noise (typically from a normal or uniform distribution) as input and transforms it into synthetic data samples. the discriminator receives both real samples from the training dataset and fake samples from the generator, outputting a probability indicating whether each sample is real or fake. the generator aims to maximize the discriminator's error rate, while the discriminator aims to minimize its own error rate. mathematically, this forms a two-player minimax game where the generator minimizes and the discriminator maximizes the same objective function. training alternates between updating the discriminator and the generator, gradually improving both networks.\n",
    "\n",
    "\n",
    "> **training algorithm**\n",
    "\n",
    "the training process alternates between:\n",
    "\n",
    "1. training the discriminator:\n",
    "   $$\\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "2. training the generator:\n",
    "   $$\\min_G V(D, G) = \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n",
    "   or equivalently:\n",
    "   $$\\max_G \\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]$$\n",
    "\n",
    "when the system reaches equilibrium, the generator produces samples indistinguishable from real data, and the discriminator outputs 0.5 for all inputs, indicating it can no longer differentiate between real and fake samples.\n",
    "\n",
    "> **challenges in gan training**\n",
    "\n",
    "training gans is notoriously difficult due to several issues. mode collapse occurs when the generator produces limited varieties of outputs, failing to capture the full data distribution. vanishing gradients can happen when the discriminator becomes too effective too quickly, providing minimal feedback to the generator. training instability manifests as oscillating losses rather than convergence. these issues have led to numerous gan variants like wasserstein gan (wgan), which uses wasserstein distance instead of jensen-shannon divergence, and spectral normalization gan (sn-gan), which stabilizes discriminator training through weight normalization.\n",
    "\n",
    "> **applications of gan**\n",
    "\n",
    "gans have revolutionized multiple fields with their ability to generate realistic data. in computer vision, they create photorealistic images, perform image-to-image translation (like converting satellite images to maps), and enhance low-resolution photos. in medicine, gans generate synthetic medical images for training algorithms and data augmentation when real samples are scarce. they've also been applied to audio synthesis for creating realistic speech and music, text generation for creating coherent passages, and even drug discovery by generating molecular structures with specific properties. perhaps most famously, deepfakes—highly realistic fake videos and images—are created using gan-based approaches.\n",
    "\n",
    "> **recent advances in gan**\n",
    "\n",
    "stylegan represents a significant advancement with its ability to generate incredibly realistic faces and control different aspects of image generation separately. biggan scaled up gan training to produce high-resolution, diverse images. cyclegan enabled unpaired image-to-image translation, allowing transformation between domains without paired examples. diffusion models, while technically different from gans, have recently outperformed them in image generation quality by gradually denoising random noise. gans continue to evolve, with research focusing on improving training stability, increasing output diversity, and extending their capabilities to new domains and applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
