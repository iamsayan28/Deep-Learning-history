{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clip (contrastive language-image pre-training)\n",
    "\n",
    "clip is a neural network model developed to connect text and images. in the context of stable diffusion, clip serves as the text encoder that transforms text prompts into embeddings that guide the image generation process.\n",
    "\n",
    "the core principle behind clip is **contrastive learning** between text and image pairs. clip learns to align text and images in a shared embedding space, where related text and images are positioned closer together and unrelated ones are farther apart.\n",
    "\n",
    "## how clip works\n",
    "\n",
    "clip consists of two encoders:\n",
    "1. a text encoder (typically a transformer)\n",
    "2. an image encoder (typically a vision transformer or cnn)\n",
    "\n",
    "these encoders map text and images into a shared, high-dimensional embedding space. during training, clip maximizes the cosine similarity between correct text-image pairs while minimizing similarity between incorrect pairs.\n",
    "\n",
    "## the main formula\n",
    "\n",
    "the contrastive loss function that clip optimizes can be represented as:\n",
    "\n",
    "$$l = -\\log \\frac{\\exp(sim(t_i, i_i)/\\tau)}{\\sum_{j=1}^{n} \\exp(sim(t_i, i_j)/\\tau)}$$\n",
    "\n",
    "where:\n",
    "- $t_i$ is the text embedding\n",
    "- $i_i$ is the corresponding image embedding\n",
    "- $sim$ is the cosine similarity function\n",
    "- $\\tau$ is a temperature parameter\n",
    "- $n$ is the batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
